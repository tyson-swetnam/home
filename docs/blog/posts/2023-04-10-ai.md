# Meet your new personal assistant, Artificial Intelligence

??? Success "Prerequisites" 

    * an open mind
    
    * healthy amount of skepticism 

    * basic ethical principles


"Have you heard of [ChatGPT](https://chat.openai.com/chat){target=_blank}?" might be the most frequently asked question in academic meetings and conversations over the last six months. 

<script type="text/javascript" src="https://ssl.gstatic.com/trends_nrtr/3316_RC01/embed_loader.js"></script> <script type="text/javascript"> trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"chatgpt","geo":"US","time":"today 12-m"}],"category":0,"property":""}, {"exploreQuery":"geo=US&q=chatgpt&hl=en&date=today 12-m","guestPath":"https://trends.google.com:443/trends/embed/"}); </script>

Large Language Models (LLMs), like ChatGPT, are [disrupting classrooms](https://www.nytimes.com/2023/02/02/learning/students-chatgpt.html){target=_blank}, [passing every major standardized test](https://doi.org/10.48550/arXiv.2303.08774){target=_blank} (often with flying colors), and casting a long shadow over [the workplace](https://doi.org/10.48550/arXiv.2303.10130){target=_blank}.

While ChatGPT and its relatives have recieved deserved skepticism about their [factual inaccuracy](https://www.nytimes.com/2023/03/14/technology/openai-new-gpt4.html){target=_blank} and criticism around their propensity for [misogyny and racism](https://www.cnn.com/2023/03/18/politics/ai-chatgpt-racist-what-matters/index.html){target=_blank}, they will inevitably impact our lives, for better and worse, over the coming months to years. We are early in this [latest Gartner Hype
Cycle](https://en.wikipedia.org/wiki/Gartner_hype_cycle), but the signs also suggest we are at the beginning of a new era where AI enters our workplaces and our homes.

## What kinds of AI do you you need to know about?

Machine Learning models have been in action for many years now. However, there are several types of AI being talked about online and in the news which all come from the same family of so-called [Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model){target=_blank}. LLMs rely upon pre-trained neural networks built from large corpus of textual data (e.g., the internet: Wikipedia, libraries of digital books, all scientific publications, and print news). LLMs can also be trained on large quantities of unlabelled text, imagery, video, or audio using self-supervised learning to produce imagery, videos, and human voices. 

### Large Language Models (LLM)

Large Language Models (LLMs) consist of a [neural-network](https://developers.google.com/machine-learning/glossary#neural-network){target=_blank} with many '[parameters](https://developers.google.com/machine-learning/glossary#parameter){target=_blank}' and '[weights](https://developers.google.com/machine-learning/glossary#weight){target=_blank}'. 

Neural networks are ...

A “parameter” is a value that the model can independently modify as it is trained. Parameters are derived from the training data upon which the model is trained. The number of parameters in the newest LLMs are typically counted in the billions to the trillions. 

"Weights" are the value by which a model multiplies another value. Weights are typically determined by the proportional value of the importance of the parameters. Weights signify the value of a specific set of parameters after self-training.

| Name | Creator | Application | Open Source? | Access |
|------|---------|-------------|--------------|--------|
|[LLaMa](https://doi.org/10.48550/arXiv.2302.13971){target=_blank} | Meta | LLM | no | free |
| [BARD ](https://bard.google.com/){target=_blank} | Google | LLM | no | free |
| [Bing](){target=_blank} | Microsoft (OpenAI) | Search | no |
| [ChatGPT](https://chat.openai.com/chat){target=_blank} | OpenAI | Chat | no | free, subscription |
| [Megatron NLG](https://doi.org/10.48550/arXiv.2201.11990){target=_blank} | NVIDIA | LLM | no | invitation |

**Open Source**

LLaMa-based AI:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html){target=_blank}

[Vicuna](https://vicuna.lmsys.org/){target=_blank}

Apple?

AWS?

### Transformers 

Types of Transformers:

Language Models for Dialog Applications (LaMDA)

[Bi-directional Encoder Representations from Transformers (BERT)](https://doi.org/10.48550/arXiv.1810.04805){target=_blank}

[Generative Pretrained Transformer (GPT)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf){target=_blank}

Transformer models have been used for years in research. In the last twelve months, the GPT-3 model, created by OpenAI has built a significant following. 

Transformer models can be used for [proteins](https://doi.org/10.1101/2020.12.15.422761){target=_blank}, or [dna model-genomes](https://doi.org/10.1093/bioinformatics/btab083){target=_blank}

### Image Generation & Segmentation

#### Stable Diffusion

[Stable Diffusion](https://huggingface.co/CompVis) was released by the [CompVis Lab](https://ommer-lab.com/) group at [Ludwig Maximilian University of Munich](https://www.lmu.de/de/index.html). 

Stable Diffusion models are available via [HuggingFace](https://huggingface.co/CompVis)

Diffusion models have two modes, forward and reverse. Forward diffusion adds random noise until the image is lost. Reverse diffusion uses Markov Chains to recover data from a Gaussian distribution, thereby gradually removing noise.

Stable Dffusion relies upon [Latent Diffusion Model (LDM)](https://doi.org/10.48550/arXiv.2112.10752)



#### Commercial platforms

[DALL·E](https://labs.openai.com/) uses GPT to create imagery from natural language descriptions

[MidJourney](https://www.midjourney.com/) uses a proprietary machine learning technology, believed to be stable diffusion, along with natural langauge descriptions in the same way as DALL·E and Stable Diffusion models. MidJourney is only available via Discord, and requires a subscription for premier access after a 30-day free trial.

[Segment-Anything (Meta)](https://segment-anything.com/), [paper :simple-pdf:](https://doi.org/10.48550/arXiv.2304.02643){target=_blank}, is a recently released image and video segmentation technology that allows you to 'clip' a feature from an image with a single click. 

[Project AIRA (Meta)](https://about.meta.com/realitylabs/projectaria/) 

[AIRA datasets](https://about.meta.com/realitylabs/projectaria/datasets/)

## Why should you care?

??? Tip "Ethics of Artificial Intelligence

   [:simple-wikipedia: Ethics of Artificial Intelligence](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence)

   https://doi.org/10.1162%2F99608f92.8cd550d1 

!!! Question "Should I be worried an AI is going to steal my job or make my diploma worthless?"

    We are still many decades away from an [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) which can learn as humans and animals do. 

    Your future career will most likely leverage AI as a digital assistant. What will be critical is staying grounded by bedrock foundations around (i) what the ethical applications of AI are, and (ii) the prohibitions of using AI in warfare or to harm humanity. 

    Other fun facts: humans are powered by our metabolism, which works out to about ~12 watts of electrical energy per brain and 80 watts of resting metabolic rate. Current LLMs require training on many thousands of GPUs for many months, each requiring thousands of watts of energy. The technological ability to create mechanical silicon based brains (Androids) or biologically artifical beings (think: the 10 foot tall alien Na'vi avatars from the Avatar movie franchise) with a similar energy budget reserved to the realm of science fiction.

Transformative events in scientific history

| Dates | Technology | Transformation | Hype Years | Years until integration in science |
|-------|------------|----------------|------------|------------------------------------| 
| 1600s | Logarithm Slide Rule Calculator | Shorthand Maths | 1640s - onward | 1640s - 1980s |
| 1800s | Babbage programmable computer & Lovelace Algorithm | | |
| 1950s | UNIVAC Computer | Mathematics | 1960s | |
| 1970s | Electric Calculator | Mathematics | 1980s | 1990s | |
| 1970s | FORTRAN | Scientific computing | 1980s | 1970s - onwards |
| 1980s | Personal Computer | Word Processing & Spreadsheets | 3 |
| 1990s | World Wide Web | The Internet! | 1991 |
| 1990s | Search Engines | Google | 1998 | |
| 2000s | iPhone | all-in-one touch screen device | 2001 - onward |
| 2010s | LLMs | 2018 |  |
| 2020s | 

Slide rules were invented by the mathematician William Oughtred around 1620 (rectangular) and 1630 (circular), based on mathematical logarithms, which were discovered by John  Napier a decade earlier <Cajori, F. (1908). Notes on the history of the slide rule. The American Mathematical Monthly, 15(1), 1-5.>

Ada Lovelace writes the first algorithm around 1842, describing an Analytical Engine to compute Bernoulli numbers for Charles Babbages computational engine. Notably, the machine is never built, its impact is not realized for another hundred years.

In 1951 the first commercial computer, UNIVAC (Universal Automatic Computer) is put into production in the 1950s with units purchased by commercial companies and research universities. By 1953 the first computer science degree program is offered at University of Cambridge, and by 1962 Purdue has formed the first computer science department. 

In the [1950s scientists proposed definitions of AI](https://web.archive.org/web/20070826230310/http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html){target=_blank}. In 1963, [Computers and Thought](https://dl.acm.org/doi/10.5555/601134){target=_blank} was published, highlighting 20 seminal papers on AI. 


The [FORTRAN (Formula Translation)](https://fortran-lang.org/en/){target=_blank} programming language is created in 1957 by John Backus team at IBM. Critically, FORTAN shortened the process of programming and made computer programming more accessible. Over 60 years later, modern FORTRAN is still one of the most popular languages for [numerical simulation modelling in high performance computing](https://github.com/lanl?language=fortran){target=_blank}.


In 2018 LLMs are released by major tech companies (Microsoft, Google, Meta (Facebook))

## Impact of AI in the workplace

Early findings (from pre-prints / non-peer reviewed research) suggest GPT and LLMs will impact over 80% of the workforce, with 20% of the workforce seeing over 50% penetration of AI technology into their work <
https://doi.org/10.48550/arXiv.2303.10130>. 

Incorporating LLMs and GPT for productivity tasks (writing, editing, outlining) can save advanced users over 50% of their effort <http://dx.doi.org/10.2139/ssrn.4375283>

Likewise using AI co-programmers (like OpenAI's [GitHub (Microsoft) CoPilot]()) can result in up to software engineering 58% improved performance in writing and generating code <https://doi.org/10.48550/arXiv.2302.06590>.

## How can you use them for your research?

We strongly encourage faculty and research teams to explore how they can incorporate LLMs like GPT-4 into their daily work in the context of developing their own Research Objects

**Allow the GPT to teach you how to code.**

By asking progressively refined prompts, or by providing example code, the GPT can explain to you what the code does, or why it is dysfunctional. It will also provide suggestions and corrections.

**AI paired-programming**

Use version control systems, like `git` (GitHub, GitLab) to track changes in your code. By leveraging GitHub's free Education accounts for students and researchers, you can gain access to OpenAI CoPilot which is natively integrated into GitHub's virtual machine CodeSpace environments. 

The CoPilot AI-paired-programmer can assist you in developing code in any programming language, or in converting legacy code from older languages into more modern ones.

**Literature Review and meta analyses**

ChatGPT has been much derided for its innacuracy and capacity to generate factually incorrect information. There are however other tools, like Paper-QA (https://github.com/whitead/paper-qa) which only rely upon the textual information provided to them (PDFs), and generate relevant citations for the contextual question asked by the user.

### General Productivity



### Code Development


### Image Analyses

Segment Anything

### Other Links

[The New York Times 5-part series on AI (April 2023)](https://www.nytimes.com/article/ai-artificial-intelligence-chatbot.html){target=_blank}

[https://www.nvidia.com/en-us/glossary/data-science/large-language-models/]()